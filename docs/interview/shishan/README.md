# 架构笔记

## 并发系列

### volatile

#### 工作内存与主内存

- 每个线程都有自己的工作内存用来存放`变量的副本`。
- `工作内存`相当于一个高速的本地缓存。
- CPU执行代码指令的时候，如果频繁从主内存加载数据，**性能**会比较差。
- 多个线程并发读写一个共享变量的时候，有可能某个线程修改了变量的值，但是其他线程不可见。

#### 作用

1. `volatile`修饰的变量，当本地工作内存修改了值之后，会**强制刷新**结果回主内存，保证主内存和本地内存的数据一致性。
2. 如果此时别的线程的工作内存中有这个变量的本地缓存，会强制让**其他线程**的工作内存中的**变量缓存直接失效过期**，不允许再次读取和使用了。
3. 当另一个线程要读取本地工作内存中被修改过的值，会发现数据已经过期。线程会从主内存中加载变量的最新的值。

#### 总结

- volatile主要作用是保证**可见性**以及**有序性**。
- 有序性涉及到**指令重排**、**内存屏障**等概念。
- volatile不能保证**原子性**。

### Java8对CAS性能的优化

#### 问题场景

- 对于一个变量信息累加的操作，不加锁的正常情况下会出现并发问题。
- 使用`synchronized`的锁量级过重。

#### Atomic原子类的底层原理

- Atomic 原子类底层用的不是传统意义的锁机制，而是无锁化的**CAS机制**，通过 CAS 机制保证多线程修改一个数值的安全性。
- CAS，`Compare and Set`，先比较再进行设置。
- 当如果在执行 CAS 的时候，发现之前获取的值跟当前的值不一样，会导致 CAS 失败，失败之后，进入一个无限循环，再次获取值，接着执行 CAS 操作。

#### Java8对CAS机制的优化

- 在大量线程高并发更新原子类的时候，可能会导致大量线程空循环、自转，**性能和效率**都不是特别好。
- `LongAdder`，通过尝试使用**分段CAS**以及**自动分段迁移**的方式来大幅度提升多线程高并发执行CAS操作的性能。

##### LongAdder

- 在 LongAdder 的底层实现中，首先有个 base 值，刚开始多线程来不停的累加数值，都是对 base 进行累加的。
- 接着如果发现并发更新的线程数量过多，就会开始施行分段 CAS 的机制，也就是内部会搞 Cell 数组，每个数组是一个数值分段。
- 大量的线程分别去对不同 Cell 内部的 value 值进行 CAS 累加操作，这样就把 CAS 计算压力分散到了不同的 Cell 分段数值中，降低了 CAS 计算压力。
- 如果要从 LongAdder 中获取当前累加的总值，就会把 base 值和所有 Cell 分段数值加起来返回给你。
- `自动分段迁移` 如果某个 Cell 的 value 执行 CAS 失败了，那么就会自动去找另外一个 Cell 分段内的 value 值进行 CAS 操作。解決了线程空旋转、自旋不停等待执行 CAS 操作的问题，让一个线程过来执行 CAS 时可以尽快的完成这个操作。

##### 总结

- **高并发中的分段处理机制实际上是一个很常见和常用的并发优化手段。**

------

### Java 锁的一些概念

#### 公平锁

- 在出现多线程竞争锁的情况下，锁的分配严格按照线程请求锁时间的先后依次调度，这样的锁实现，称为公平锁。
- 锁的效率略低。
- 每个新来请求锁资源的线程不会直接进行 CAS 操作去获取锁，而且判断当前等候队列是否存在等待线程。如果存在等待线程，当前线程会直接插入队尾。

#### 非公平锁

- 在出现多线程竞争锁的情况下，请求锁的线程可以直接通过 CAS 操作去申请锁资源，锁的分配不严格按照线程请求锁时间的先后依次调度，这样的锁实现，称为非公平锁。
- 效率高于公平锁。
- 每个新来请求锁资源的线程都会进行 CAS 加锁操作，可能出现插队成功现象，锁的利用率更高。
- **java 并发包中很多锁的默认行为都是非公平锁。**

#### 重入锁

- 当一个线程持有锁的时候，可以访问任意需要该锁的资源和方法。
- ReentrantLock、ReentrantReadWriteLock都是可重入锁的实现。
- 发生锁的重入行为时，`AQS#state`的值 + 1。
- 释放锁的时候，`AQS#state`递减，直到 `state=0`，设置 owner 为 null。

#### 非重入锁

- 锁不可以被任意线程重复持有，获取锁的 CAS 操作的预期 state 值始终为 0。
- 由于不可重入，释放锁的时候，直接设置 `state=0`、`owner=null`。
- `NonReentrantLock` 是不可重入锁的实现。

### AQS 的理解

#### AQS 简介

- Java 高并发的基石。
- 全称 `AbstractQueuedSynchronizer`，抽象队列同步器。
- Java 并发包下很多 API 都是包含了 AQS 的一种实现，都是基于 AQS 实现了加锁、释放锁等功能。
- ReentrantLock、ReentrantReadWriteLock 底层都是基于 AQS。

#### ReentrantLock 底层原理

##### AQS 核心变量

- `state`
  - int 类型。
  - 表示加锁的状态。
  - 初始状态下 state 的值是 0。
- `exclusiveOwnerThread`
  - Thread 类型。
  - 用来记录当前加锁线程是哪个线程。
  - 初始化状态下，这个变量是 null。
- `head`、`tail`
  - 都是 Node 类型。
  - 属性包含 `next` 和 `prev`  指向前一个和后一个 Node 对象。
  - 相当于维护在 AQS 内部的一个等待队列，存储加锁失败的线程。

##### 加锁过程

- 通过 CAS 修改 `AQS#state` 的值为 1。
- CAS 修改成功说明锁未被占用，则更新 `exclusiveOwnerThread` 为当前线程。
- CAS 修改失败说明锁已经被占用，将当前线程封装为 `Node` 添加到等候队列的队尾，并完成线程自挂起`Thread.currentThread().interrupt()`操作。

##### 释放锁过程

- 通过比较 `exclusiveOwnerThread`和当前线程，判断当前线程是否持有当前锁，如果未持有锁，抛出 `IllegalMonitorStateException` 异常。
- 如果当前线程持有锁，`AQS#state` 进行减一操作，当 `AQS#state` 为 0，设置 `exclusiveOwnerThread` 为 null 并唤醒等待队列队列头的线程。
- 释放锁成功，唤醒等候线程队列对头的线程进行 CAS 加锁操作，**如果是非公平锁，此时被唤醒的头部线程未必一定可以获得到锁，可能被外来的线程直接抢到锁**。

------

### 微服务注册中心的读写锁优化

#### 读写锁

- 所谓的读写锁，就是将一个锁拆分为**读锁**和**写锁**两个锁。
- 加锁的时候，可以选择加写锁，也可以选择加读锁。
- **加读锁的操作是不受影响的，可以多个线程同时申请到读锁。**
- 当一个线程加了写锁，那么其他线程就不能加写锁了，**同一时间只能允许一个线程加写锁**。
- **当一个线程加了写锁，其他线程就不能加读锁**。
- **如果任意一个线程加了读锁，此时其他线程是不可以加写锁的**，写入操作需要等待读锁完全被释放。
- **读写锁是非常适合读多写少的场景的。**

#### 服务注册表

- 注册中心会维护一张服务注册表，用来存储服务及服务实例端口、服务地址等数据。
- 由于注册中心的业务场景特点，在内存中的服务注册表天然存在读写并发问题，可能存在同时多线程读、多线程写的问题。

#### 优化流程

##### synchronized

- 简单粗暴的给读写方法都添加上 `synchronized` 关键字。
- 虽然能达到数据的读写安全，但是并发性能很低，并不推荐。

##### 读写锁

- 通过使用读写锁，保证读写业务的安全和高效。
- 服务注册表的业务场景，大部分时候都是读操作，所以使用**读锁**可以让大量的线程同时来读数据，**不需要阻塞不需要排队，保证高并发读的性能是比较高的**。
- 服务注册表的业务场景，**存在少量的写入场景，使用写锁**，保证写入数据过程中，不会存在**并发修改数据**或者**读取数据**的问题。

##### 多级缓存

- 通过**多级缓存**的机制，尽量在写数据的期间还保证可以继续读数据，同时在大量加读锁的时候，降低阻塞写数据加写锁过长时间的情况。

------

### 重要数据的双缓冲区+异步刷盘方案

#### 背景

- 重要核心的业务数据持久化的降级方案。
- 数据正常会以消息的方式发送的 MQ 集群中，由于 MQ 天然的抗高并发性，不需要额外的性能优化设计。
- 为了保证重要的业务数据不丢失，需要对 MQ 集群故障设计一套降级的解决方案。

#### 方案

- 当 MQ 集群故障后，使用**内存双缓冲区+异步刷盘**方案进行业务数据的本地化存储的方案。

- 内存中双缓存区分为 ready 区、current 区。
  - ready 区域中数据是即将要进行刷入本地磁盘文件的数据。
  - current 区域是用来存储来自于当前系统请求中业务数据。
  - ready 区域在每次刷盘后清空内容。
  - 当 current 区域中的数据达到额定容量且 ready 区域为空的时候，交换分区。如果 ready 区域正在进行刷盘操作，不为空时候，系统会挂起后续的写入请求。
- 异步刷盘使用Java NIO 的 API，高性能 append 方式的写入到本地磁盘文件。
- 本地磁盘文件也有自己的文件大小限制、文件新建策略。

#### 流程图

![双缓冲区+异步刷盘](README-image/%E5%8F%8C%E7%BC%93%E5%86%B2%E5%8C%BA+%E5%BC%82%E6%AD%A5%E5%88%B7%E7%9B%98.png)

#### 并发优化

- 双缓冲区的默认值是 512K，可以通过简单的性能测试，
- 线上生产环境出现极高并发的时候，本地刷盘需要一定的时候，导致缓冲区不能释放，大量线程挂起阻塞，造成系统无响应。
- 解决方案是调整缓冲区的大小，从 512K 调整到 10MB。

------



### RabbitMQ 消息生产者的可靠投递

#### 场景

- 消息在投递的过程中，由于网络问题出现丢失。
- 消息被接收后，在消息中间件的内存中没有写入磁盘就产生了宕机。

#### 解决方案

- 生产者端开启 `confirm` 模式，通过 ack 确认消息已经正常的完成投递和持久化，否则根据业务需要进行重新投递。
- 在收到 MQ 回传的 ack 之前，要缓存消息的内容，便于重新投递。
- 开启了`confirm` 模式之后，每次消息投递也同样是有一个 `delivery tag` 的，也是起到**唯一标识一次消息投递**的作用。
- Rabbit MQ接收消息后，如果产生内部错误，会通过 `nack` 通过生产者，生产者根据 nack 返回的内容进行投递失败处理。
- 如果某条消息很长时间都没有回传 `ack/nack`，那可能是极端意外情况发生数据产生丢失，可以进行重新投递。

#### confirm机制投递消息的高延迟性

- 一旦启用了`confirm` 机制投递消息到MQ之后，MQ是不保证什么时候会给你一个 `ack` 或者 `nack` 的。
- RabbitMQ采用异步批量刷盘的方式将消息批量从内存持久化到硬盘的方式兼顾了高并发写入的吞吐量和性能的方案。因此，在开启了 `confirm` 机制后，`ack` 或者 `nack` 消息可能出现延迟。
- 如果采用每条消息都强制刷盘操作的话，会大大降低集群的吞吐量，不建议 `fsync`。

#### 问题分析

##### 消息缓存介质的选择

- 等待 `ack/nack` 期间，缓存的消息不建议使用内存，高并发场景下，可能因为缓存过多的消息而导致内存溢出。
- KV 存储是理想的选择，kV 存储承载高并发能力极强，且 KV 操作性能很高，推荐 Redis 等。

##### 写消息方式的选择

- 绝对不能以**同步写消息 + 等待ack的方式**来投递，那样会导致每次投递一个消息都同步阻塞等待几百毫秒，会导致投递性能和吞吐量大幅度下降。
- **投递消息之后等待 ack 的过程必须是异步的。**消息投递出去之后，投递线程就可以正常返回，每个消息的异步回调，是通过在 `channel` 注册一个 `confirmListener` 实现的。
  - 收到一个消息 ack 后，就从 KV 存储中删除这条临时消息。
  - 收到一个消息 nack 后，就从 KV 存储提取消息重新投递。
  - 通过对 KV 存储里的消息做监控，过滤出超过一定时长没收到 ack的消息，进行主动重发。

#### 消息中间件全链路的数据可靠要点

##### 保证生产端投递出去的消息不丢失

- 避免消息在半路丢失。
- 避免消息在 MQ 内存中因为宕机等原因丢失。
- 结合不同的 MQ 提供的 API 保证数据的可靠性。

##### 保证MQ自身消息不丢失

- 开启持久化功能，将消息写入到磁盘中。
- 使用镜像集群，备份多套消息数据。

##### 保证消费端的消费消息不丢失

- 关闭自动 ack，开始手动 ack 机制。

------

### Java 并发性能优化的一些建议

#### 双缓冲机制

- 解决中间件系统、大数据系统经常需要涉及的场景就是**核心数据写入磁盘**的核心数据缓存问题。
- 在内存中开辟两块空间，向其中一块缓冲器内写数据，当这块缓冲区满额后，后来的写入操作转移到另一块缓冲区。当前缓冲区开始进行异步的刷盘操作。
- 通过两块缓冲区交替缓存、刷盘的工作方式，提供系统的并发性能。
- 假如只用单块缓冲，必然导致读内存数据，刷入磁盘的过程，长时间占用锁。进而导致大量线程卡在锁的获取上，无法获取到锁，然后无法将数据写入内存。这里是经典的用**空间换时间**的解决方案。

#### 内存缓冲区分片机制+分段加锁机制

- 如果写入方法只有一把锁控制的话，会导致所有的写入操作都要串行化，系统的吞吐量会急剧降低。
- 分段锁可以解决串行化严重的问题，**对内存双缓冲机制引入分段加锁机制，也就是将内存缓冲切分为多个分片，每个内存缓冲分片就对应一个锁。**
- **获取缓存分片的方法可以定制负载均衡策略。**

#### 内存 + 磁盘并行写机制

- 缓冲区满了开始进行刷盘操作，如果没有释放锁的话，虽然完成了缓冲区的切换，但是锁没有被释放，其他线程是不能获取锁而进行写入操作。
- **在缓冲区满后，完成切换，要释放当前分片的锁，再由当前线程完成磁盘写入工作。此时磁盘的刷写和内存的写入，完全可以并行同时进行。**
- 并行写入的机制大幅度降低了锁占用的时间，这是 java 并发锁优化的一个非常核心的思路。

### 20W 并发服务热点数据

>热点缓存的架构优化问题。

#### 缓存集群的必要性

- 互联网架构中，往往通过缓存集群来减少对数据库的读压力。

- **缓存集群的并发能力是很强的，而且读缓存的性能是很高。**

#### 处理每秒2w的读写请求

假定每秒 2w 的读写请求中，90%是数据读取，10%是数据写入的场景。

##### 数据库方案

- 分库分表 + 读写分离。
- 三个主库承载每秒 2000 的写入请求。
- 每个主库挂接三个从库，共计 9 个从库承载每秒 1.8w 的读请求。
- 需要合计 12 台 高配置的数据库服务器。

##### 缓存集群方案

- 采用两主两从的缓存集群。
- 主节点用来写入缓存，从节点用来读缓存。
- 两个从节点可以承担每秒 1.8w 的大量读请求。
- 三个数据库主库承担每秒 2000 的写入和少量的读取。
- 合计需要 7 台服务器。

#### 热点缓存的超量访问

- 瞬时超大量的请求访问同一个 key 的缓存数据，可能造成存储对应 key 的缓存服务器崩溃。
- 缓存服务器崩溃后，请求部分来到数据库，数据库查询到数据后，缓存到缓存集群的另一个服务器上，剩余的请求又向新的缓存服务器请求数据，造成新的缓存服务器的崩溃。
- 依次轮转，可能导致整个缓存集群的崩溃。

#### 基于流式计算技术的缓存热点自动发现

- 对于超级热点缓存，系统需要能够在热点缓存突然发生的时候，直接发现他，然后瞬间实现毫秒级的**自动负载均衡**。
- 在请求到达时，可以基于大数据领域的流式计算技术来进行**实时数据访问次数的统计**，`storm`、`spark streaming`、`flink`。
  - **流式计算系统会将请求分布在不同的机器中进行计算，后汇总数据做全局计算，因此不会存在超级并发问题。**
- **通过实际数据访问次数计算，根据业务情况发现超级热点数据，并将热点数据记录下**，比如记录在  `zookeeper` 中。

#### 热点缓存自动加载为JVM本地缓存

- 业务系统通过对 `zookeeper` 指定的热点缓存对应的 `znode` 进行监听，会感知到热点数据的产生。
- 此时业务系统将要缓存数据从数据库中加载出来，放在**系统内部的本地缓存**里即可。
  - 本地缓存使用 `ehcache`、`hashmap`都可以。
  - 只要就是从缓存集群中，升级为系统的本地缓存即可。
- 需要评估好热点数据，系统的本地缓存是不能存储过多数据的。

#### 限流熔断保护

- 业务系统内部，需要对热点数据访问添加一个限流熔断保护措施。
- 一单请求超过设定的阈值，就开启熔断策略，返回空白页，保护后端的缓存集群和数据库。

####  图示

![缓存方案](README-image/%E7%BC%93%E5%AD%98%E6%96%B9%E6%A1%88.png)

------

### 日活百万系统的数据库设计

#### 服务器并发能力

- 16 核 32 G的数据库服务器，每秒请求支撑不要超过 2000。

#### 分库

- 多个服务器的数据库实例下创建相同的库。
- 写入数据的时候借助于 `sharding-jdbc`、`mycat` 中间件。
- 根据业务规则进行数据的分库存储，比如订单 ID 的 hash 后根据库的数量进行取模。
- 通过 ID 来查询的时候，同样根据 ID 的 hash 进行取模锁定要查询的库。

#### 分表

- 通过分表，大量分表，解决分库后单表数据依然过大的问题。
- 此时写入分表的数据需要经过两次路由才能锁定具体的数据库的具体的表中。
- 大量分表的策略保证可能未来10年，每个表的数据量都不会太大，这可以保证单表内的SQL执行效率和性能。

#### 读写分离

- 通过数据库的主从模式，实现读写分离。
- 写入主库的时候，会自动同步数据到从库上去，保证主库和从库数据一致，查询的时候都是走从库去查询。
- 生产中，大部分的还是读请求，通过扩容从库，可以达到支持更多的读请求。
- 对同一个表，如果你既写入数据（涉及加锁），还从该表查询数据，可能会牵扯到锁冲突等问题，无论是写性能还是读性能，都会有影响。

#### 总结

- 大量分表保证每个表的数据量别太大，读写分离实现主库和从库按需扩容以及性能保证。

------

### 百万连接的系统架构设计

#### 长连接

- 系统建立连接后，连接长期持有，多次请求无需花费额外的开销进行连接创建。
- 可以提供更快的响应速度。

#### 线程资源的浪费

- 根据线上的生产经验，一般4核8G的标准服务用的虚拟机，自己开辟的工作线程在**一两百**个就会让CPU负载很高了，**最佳的建议就是在几十个工作线程就差不多**。
- 期望每个服务实例来维持上万个线程，那几乎是不可能的。

#### Reactor 多路复用

-  **Reactor 多路复用**模型是支持高并发的常用架构策略，kafka 等中间件就使用该策略。
- 多路复用体系中包含 `acceptor` 线程，基于底层操作系统的支持，实现连接请求监听。
- 如果有某个设备发送了建立连接的请求过来，那么那个线程就把这个建立好的连接交给 `processor` 线程。
- 每个 `processor` 线程会被分配N多个连接，**一个线程就可以负责维持N多个连接**，基于底层操作系统的特殊机制的支持监听N多连接的请求，可以让一个线程支持多个连接了，不需要一个连接一个线程来支持。
- `processor` 线程仅仅是接收请求和发送响应。所有的请求都会入队列排队，交给后台线程池来处理。
- **工作线程池里的线程会从请求队列里获取请求，处理请求，接着将请求对应的响应放到每个 processor 线程对应的一个响应队列里去。**
- **processor 线程会把自己的响应队列里的响应发送回给客户端**。

![多路复用机制](README-image/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E6%9C%BA%E5%88%B6.png)

------

## 微服务

### Spring Cloud 底层原理

#### Eureka

- `Eureka` 是微服务架构中的注册中心，专门负责服务的注册与发现。
- `Eureka Client` 会在服务启动的时候，将服务的地址、端口、名称等信息发送到 `Eureka Server`。
- `Eureka Server`  收集服务发送过来的注册信息，完成注册、维护服务器端的注册表。
- 每个 `Eureka Client`  客户端本地缓存一份注册表信息，保存了各个服务所在的机器和端口号。

#### Feign

- `Feign` 是一套远程调用组件，完成服务的远程调用。
- **Feign的一个关键机制就是使用了动态代理**。
- 在某个接口定义了 `@FeignClient` 注解，Feign就会针对这个接口创建一个动态代理。
- Feign 的动态代理会根据你在接口上的 `@RequestMapping` 等注解，来动态构造出你要请求的服务的地址。
- Feign 根据方法的参数、返回值在方法被调用的时候完成发起请求、解析响应。

#### Ribbon

- `Ribbon` 的作用是**负载均衡**，会帮你在每次请求时选择一台机器，均匀的把请求分发到各个机器上。
- `Ribbon` 的负载均衡默认使用的最经典的 `Round Robin 轮询算法`，将请求依次分配到每个服务器。
- `Ribbon` 是和 `Feign` 以及 `Eureka` 紧密协作，完成工作的。
  - `Ribbon` 会从 `Eureka Client` 里获取到对应的服务注册表，也就知道了所有的服务都部署在了哪些机器上，在监听哪些端口号。
  - `Ribbon` 就可以使用默认的 `Round Robin` 算法，从中选择一台机器。
  - `Feign` 就会针对这台机器，构造并发起请求。

#### Hystrix

- `Hystrix` 是隔离、熔断以及降级的一个框架。
- `Hystrix` 会维护很多个小小的线程池，发起请求是通过 `Hystrix` 的线程池来走的，**不同的服务走不同的线程池，实现了不同服务调用的隔离，避免了服务雪崩的问题**。
- **服务降级的时候，执行降级逻辑，往本地服务数据库中或者日志中记录必要的业务信息，等待服务回复后，恢复数据。**

#### Zuul

- `Zuul` 是微服务网关，**负责网络路由**。
- **所有请求都往网关走，网关会根据请求中的一些特征，将请求转发给后端的各个服务。**
- 有了网关之后，可以做**统一的降级、限流、认证授权、安全**。

------

### Eureka 如何承载每日千万级访问

#### Eureka 的服务注册与心跳管理

- 各个服务内的 `Eureka Client` 组件默认是**每隔 30 秒**向  `Eureka Server` 获取最新的服务器应用列表信息。
- Eureka 的心跳机制，各个 `Eureka Client` **每隔 30 秒**会发送一次心跳到 **Eureka Server**。
- 日请求在千万级访问量的情况下，实际上 `Eureka Server` 每秒钟承载的请求大概在 200~300 次。
- 通过设置一个适当的拉取注册表以及发送心跳的频率，可以保证大规模系统里对 `Eureka Server` 的请求压力不会太大。

#### 服务端注册表存储结构

- `Eureka Server` 存储注册信息的容器选择的是 `CocurrentHashMap`。
- 注册表直接基于**纯内存**，即在内存里维护了一个数据结构。
- 各个服务的注册、服务下线、服务故障，全部会在内存里维护和更新这个注册表。
- 各个服务每隔 30 秒拉取注册表的时候，`Eureka Server` 就是直接提供内存里存储的**有变化的注册表数据**。
- **维护注册表**、**拉取注册表**、**更新心跳时间**，全部发生在内存里。这是 `Eureka Server` 非常核心的一个点。
- 注册表的结构 `CocurrentHashMap<String, Map<String, Lease<InstanceInfo>>>`。
  - `CocurrentHashMap` 的 `key` 是服务的名称。
  - `CocurrentHashMap` 的 `value` 是服务的多个实例。
  - `Map<String, Lease<InstanceInfo>>` 的 `key` 是**服务实例的id**。
  - `Map<String, Lease<InstanceInfo>>` 的 `value` 是 `Lease类`，泛型是 `InstanceInfo`。
    - `Lease` 维护每个服务器最近一次的心跳时间。
    - `InstanceInfo` 存储服务实例的具体信息。

#### 服务端多级缓存机制

- 假设 Eureka Server 部署在 4 核 8G 的普通机器上，**基于内存来承载各个服务的请求**，每秒可以轻松的处理几百请求。
- `Eureka Server` 为了**避免同时读写内存数据结构造成的并发冲突问题**，采用**多级缓存机制**来进一步提升服务请求的响应速度。
- **拉取注册表**
  - 首先从 `ReadOnlyCacheMap` 里查缓存的注册表。
  - 若没有，就找 `ReadWriteCacheMap` 里缓存的注册表。
  - 如果还没有，就从**内存中获取实际的注册表数据。**
- **修改注册表**
  - 在内存中更新变更的注册表数据，同时**过期掉ReadWriteCacheMap**。
  - 此过程不会影响 `ReadOnlyCacheMap` 提供查询注册表功能。
  - 一段时间内（默认30秒），各服务拉取注册表会直接读 `ReadOnlyCacheMap`。
  - 30秒过后，`Eureka Server` 的后台线程发现 `ReadWriteCacheMap` 已经清空了，也会清空  `ReadOnlyCacheMap` 中的缓存。
  - 下次有服务拉取注册表，又会从内存中获取最新的数据了，同时填充各个缓存。

#### 多级缓存机制的优点

- **尽可能保证了内存注册表数据不会出现频繁的读写冲突问题。**
- **进一步保证对 Eureka Server 的大量请求，都是快速从纯内存走，性能极高。**

![Eureka多级缓存注册表](README-image/Eureka%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%E6%B3%A8%E5%86%8C%E8%A1%A8.png)

------

### 每秒上万并发下的 Spring Cloud 参数优化实战

